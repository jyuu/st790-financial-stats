\documentclass[11pt,]{article}
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\newcommand*{\authorfont}{\fontfamily{phv}\selectfont}
  \usepackage[]{mathpazo}
  
  
  \usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}



\usepackage{abstract}
\renewcommand{\abstractname}{}    % clear the title
\renewcommand{\absnamepos}{empty} % originally center

\renewenvironment{abstract}
{{%
  \setlength{\leftmargin}{0mm}
  \setlength{\rightmargin}{\leftmargin}%
}%
  \relax}
{\endlist}

\makeatletter
\def\@maketitle{%
  \newpage
  %  \null
  %  \vskip 2em%
    %  \begin{center}%
    \let \footnote \thanks
  {\fontsize{18}{20}\selectfont\raggedright  \setlength{\parindent}{0pt} \@title \par}%
}
%\fi
\makeatother


  
  
  \setcounter{secnumdepth}{0}

      \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
        
    
    \title{ST790: Quantopian Final Project  }
  
  
  
  \author{\Large \vspace{0.05in} \newline\normalsize\emph{}  }
  
  
  \date{}

\usepackage{titlesec}

\titleformat*{\section}{\normalsize\bfseries}
\titleformat*{\subsection}{\normalsize\itshape}
\titleformat*{\subsubsection}{\normalsize\itshape}
\titleformat*{\paragraph}{\normalsize\itshape}
\titleformat*{\subparagraph}{\normalsize\itshape}


  
      
  
  \newtheorem{hypothesis}{Hypothesis}
\usepackage{setspace}

\makeatletter
\@ifpackageloaded{hyperref}{}{%
  \ifxetex
  \PassOptionsToPackage{hyphens}{url}\usepackage[setpagesize=false, % page size defined by xetex
                                                 unicode=false, % unicode breaks when used with xetex
                                                 xetex]{hyperref}
  \else
    \PassOptionsToPackage{hyphens}{url}\usepackage[unicode=true]{hyperref}
  \fi
}

\@ifpackageloaded{color}{
  \PassOptionsToPackage{usenames,dvipsnames}{color}
}{%
  \usepackage[usenames,dvipsnames]{color}
}
\makeatother
\hypersetup{breaklinks=true,
bookmarks=true,
pdfauthor={ ()},
pdfkeywords = {},  
pdftitle={ST790: Quantopian Final Project},
colorlinks=true,
citecolor=blue,
urlcolor=blue,
linkcolor=magenta,
pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\setlength{\abovedisplayskip}{.2pt}
\setlength{\belowdisplayskip}{.2pt}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{chngcntr}
\usepackage{multicol}
\usepackage{lscape}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{multirow}
\newtheorem{theorem}{Theorem}
\usepackage[linesnumbered,algoruled,boxed,lined,commentsnumbered]{algorithm2e}
\usepackage{bm}
\newcommand{\blandscape}{\begin{landscape}}
\newcommand{\elandscape}{\end{landscape}}


% add tightlist ----------
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

% \pagenumbering{arabic}% resets `page` counter to 1 
%
% \maketitle

{% \usefont{T1}{pnc}{m}{n}
\setlength{\parindent}{0pt}
\thispagestyle{plain}
{\fontsize{18}{20}\selectfont\raggedright 
\maketitle  % title \par  

}

{
  \vskip 13.5pt\relax \normalsize\fontsize{11}{12} 
  \textbf{\authorfont } \hskip 15pt \emph{\small }   
  
}

}






\vskip 6.5pt


\noindent  \section{Introduction}\label{introduction}

As fund managers, we were tasked with constructing a cross-sectional,
long-short US equity strategy on
\href{https://www.quantopian.com/}{Quantopian}. There were not many
explicit constraints on the specific strategy we utilized, but it must
pass the following criteria: (i) trade liquid stocks, (ii) have no more
than 5\% of capital invested in any one asset, (iii) have no more than
10\% net dollar exposure, (iv) achieve mean daily turnover between 5\%
and 65\% over a 63-trading-day rolling window, (v) attain gross leverage
between 0.8x and 1.1x, (vi) have low correlation to the market, (vii)
have less than 20\% exposed to each of the 11 sectors as defined on
Quantopian, and (viii) result in positive returns. While the last return
criteria was not a constraint we included in our optimization, we did
design our algorithm with the rest of the seven criteria in mind.

\section{Trading Strategy}\label{trading-strategy}

The backtests we describe below are derived from the following trading
algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Once a week, we choose a universe of liquid assets from
  \(\texttt{QTradeableStocksUS}\) that pass the following filters:

  \begin{itemize}
  \tightlist
  \item
    It is not trading within 2 days of any earnings announcements as
    assets are generally more volatile within these dates.
  \item
    It has not been announced as an acquistion target. To further reduce
    any possible volatility, we avoid acquisition targets as they often
    pose huge risk to quant strategies.
  \item
    We are able to calculate a 5 day moving average of the
    bull-minus-bear signal from the \(\texttt{StockTwits}\) API.
  \end{itemize}
\item
  Every day, we build an alpha vector for the universe of liquid assets
  filtered from our step above. The alpha model we use is quite simple:
  we rank the assets by its bull-to-bear intensity, averaged over the
  past 5 days as evaluated from \(\texttt{StockTwits}\), and find a set
  of new portfolio weights that maximizes the sum of each asset's weight
  times this alpha value. Our objective defined in
  \(\texttt{MaximizeAlpha}\) is thus simply a function of this sentiment
  datastream as we believe this ranking is similar to expected returns
  of each asset. As a result, our routine effectively goes long on
  assets with high bullish signal and short on those with a high bearish
  signal.\\
\item
  Once a week, we calculate the portfolio that maximizes the
  alpha-weighted sum of our position sizes, subject to the following
  constraints:

  \begin{itemize}
  \tightlist
  \item
    Our portfolio maintains a gross leverage of, or less than, 1.0x.
  \item
    Our portfolio has no more than 5\% in any single asset.
  \item
    Our portfolio does not pass mean daily turnover of 80\%.
  \end{itemize}
\end{enumerate}

With this simple strategy, we achieve the following leaderboard results
at the end of our one-month trading period from November \(1^{st}\) to
November \(30^{th}\):

\newpage

\begin{table}[ht]
\small
\centering
\begin{tabular}{rllr}
  \hline
 & Metric & Our Result & Overall \\ 
  \hline
1 & rank & 64 & - \\ 
  2 & name & Gray Fox & - \\ 
  3 & score & 0.502 & 0.36 \\ 
  4 & max\_beta\_to\_spy\_126day & 0.076 & 0.14 \\ 
  5 & max\_cumulative\_common\_returns & 0.009 & 0.04 \\ 
  6 & max\_leverage & 1.047 & 1.05 \\ 
  7 & max\_max\_drawdown & 0 & -0.00 \\ 
  8 & max\_net\_dollar\_exposure & 0.032 & 0.04 \\ 
  9 & max\_total\_returns & 0.025 & 0.14 \\ 
  10 & min\_total\_returns & -0.007 & -0.02 \\ 
  11 & max\_turnover & 0.905 & 1.07 \\ 
  12 & max\_volatility\_126day & 0.044 & 0.06 \\ 
   \hline
\end{tabular}
\end{table}

Key to our strategy is the output from the \texttt{bull\_minus\_bear}
API call as it is this signal that is fed into the optimization function
and this result that determines the order size of each asset. While we
do not have exact clarity on the natural language processing engine that
calculates the bullish intensity and bearish intensity of a stock, we do
know how the \texttt{bull\_minus\_bear} signal arises; traders attached
either a bull emoji or a bear emoji to any message they release on the
\texttt{StockTwits} platform as well as a ticker symbol that identifies
which asset is under discussion. While traders attached a clear label to
the asset of interest, \texttt{StockTwits} also has an in-house
proprietary algorithm that processes some of the language in the message
to ascribe an intensity level of the bull or bear indicator. Messages
across the \texttt{StockTwits} platform is thus aggregated to arrive a
sentiment score that is a function of subtracting the bearish intensity
from the bullish intensity result.

\subsection{Choice of the sentiment
score}\label{choice-of-the-sentiment-score}

In addition to the ease of implementation, we chose to rely on the
sentiment factor as it seems to have some good characteristics, namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Predictive alpha} We calculated the mean information
  coefficient using the built-in function in \texttt{alphalens} and
  found our sentiment signal matches the direction of actual asset
  returns. As shown in the top-left of the figure below, we only get the
  full exposure of this factor 5 days after this signal becomes
  available. In fact, prior to 5 trading days, it appears the signal
  hurts us, in that its forecast is negative. Counterintuitively, after
  we cross 5 trading days, it appears our sentiment factor gains more
  predictive power. This is further corroborated in the top-right graph
  in that returns are highest when the signal is delayed by four days.
  Given this characteristic, we may want to build greater exposure to
  this factor since it still benefits us after 10 trading days. We can
  also leverage the positive effects of this factor by increasing our
  turnover constraints as it appears it does not hurt our portfolio to
  keep in there for a longer period of time. Nevertheless, the mean
  information coefficient across the time frame displayed still lingers
  around 0, suggesting the forecasting benefits provided by our
  sentiment factor may be no better than the results we get from
  randomly selecting our asset weights.
\item
  \textbf{Low exposures} We quantified the exposures via the
  \texttt{perf\_attrib} function in \texttt{pyfolio}. As shown in the
  bottom-left, our sentiment factor appears to have relatively low
  exposures throughout, with most of the returns from volatility risk.
  However, there does appear to be persistent exposure to value and
  short term reversal, in that it is shifted to the left of zero; and
  persistent exposure to size and momentum, in that it is shifted to the
  right of zero. Ideally, we would choose a factor that doesn't display
  this skew, but the skew does not appear too large. We also benefit
  from the fact that the width of each of our box and whisker plots are
  not that wide, so our exposures do not vary much over time. Yet, this
  analysis was conducted only over the available sentiment signal in
  2018, so there may be other extended periods prior to 2018 in which
  our factor over or underperforms. Given what we have here, we may want
  to identify asset classes to include in our portfolio that would
  offset the exposure risks identified above.
\end{enumerate}

Unfortunately, when we examine our exposures through cumulative returns
and volatility, we find that our sentiment factor may not be as strong
as we desired. As shown by the left bar graph (bottom-right), most of
our returns are obtained from exposure to common risk factors, and, as
shown in the right bar graph, are in fact driven by these common risk
factors. Ideally, we would like to strip away the contribution from
these common risk factors as portfolio performance from this exposure
can easily be replicated from ETFs or other easy, cost-effective
methods.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{/home/joyce/workspace/st790-financial-stats/final/download2} 

}

\caption{Examination of (top-right) information coefficient averaged over all possible asset returns, (top-left) cumulative returns when factor is delayed, (bottom-left) risk exposures, and (bottom-right) cumulative returns and volatility.}\label{fig:unnamed-chunk-2}
\end{figure}

\newpage 

\section{Backtesting}\label{backtesting}

Despite the contradictory results from our initial explorations of our
sentiment index appeared to indicate, given the extensive literature
that exists on sentiment data predicting stock price, we forged ahead
and backtested our trading strategy on Quantopian. The backtests for the
period between January 2015 and August 2015 are shown below. Attempts
were made to obtain backtest results for a longer, more representative,
period that reflects the market today, but there were issues obtaining
it through the backtesting platform on \texttt{Quantopian}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{/home/joyce/workspace/st790-financial-stats/final/structural} 

}

\caption{Total and specific returns of our trading strategy between January and August 2015.}\label{fig:unnamed-chunk-3}
\end{figure}

While our exploratory analyses in Figure 1 suggests most of our returns
are from common risk factors, when backtested in the period of 2015, we
find that most of our cumulative returns are from specific
returns--returns that are attributable to our trading strategy
(Mackenzie \protect\hyperlink{ref-Quant}{2018}). Moreover, as desired,
our algorithm can be characterized as a momentum strategy, which has
time over time demonstrated profitability (Jegadeesh and Titman
\protect\hyperlink{ref-Jeg}{2002}). In fact, a number of authors, Hong
and Stein (\protect\hyperlink{ref-Hong}{2002}), has shown behavioral
models that explain how momentum profits arise from how investors
interpret data. They even go as far as demonstrating the advantages in
longer holding periods, resulting from delayed overreaction to news that
further push the prices of winners (or pull the prices of losers) from
their long-term values as our initial analyses imply.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{/home/joyce/workspace/st790-financial-stats/final/style} 

}

\caption{Exposure to investing styles assessed from a rolling 63-day mean. The 5 different investing styles presented are: momentum (light blue), volatility (dark blue), size (orange), value (gray) and short term reversal (purple).}\label{fig:unnamed-chunk-4}
\end{figure}

In Figure 4, our predominantly momentum-based strategy appears to have
negative beta to the market. While we should seek to build an algorithm
that has low beta in general, this negative beta might actually benefit
us during our set trading window of November since the market has been
trending down for the entire month of October. As this beta from our
simple OLS estimator is particularly sensitive to different time
periods, we should also have considered using more sophisticated
estimation methods like a shrinkage estimator where we could include
some prior belief on how we think the asset beta should behave.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{/home/joyce/workspace/st790-financial-stats/final/beta} 

}

\caption{Rolling beta to SPY calculated on a 6 month rolling basis. A small window is shown since our backtest range is relatively short, and this metric can only be assessed 6-months after our initial backtest date.}\label{fig:unnamed-chunk-5}
\end{figure}

Given the constraints we fed in our \texttt{optimize} API call, we are
able to achieve the desired leverage, turnover and net dollar exposure
in order for our trading strategy to remain on the \texttt{Quantopian}
platform. As shown in Figure 5, the leverage remains between the 0.8x
and 1.1x thresholds. In Figure 6, the absolute net dollar exposure is
below the 10\% requirement. And in Figure 7, the turnover is below 65\%
cutoff. Since we pass all the constraints, we submit our sentiment based
strategy to the contest.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{/home/joyce/workspace/st790-financial-stats/final/leverage} 

}

\caption{Gross leverage calculated as a function of the current portfolio value to its capital base.}\label{fig:unnamed-chunk-6}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{/home/joyce/workspace/st790-financial-stats/final/netdollar} 

}

\caption{Net dollar exposure calculated by total value of our strategy's long and short positions.}\label{fig:unnamed-chunk-7}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{/home/joyce/workspace/st790-financial-stats/final/turnover} 

}

\caption{Total turnover rate calculated over a rolling window of 63 days.}\label{fig:unnamed-chunk-8}
\end{figure}

\section{Evaluation}\label{evaluation}

The following displays our portfolio performance between Nov 1 and Nov
30:

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{/home/joyce/workspace/st790-financial-stats/final/submission} 

}

\caption{Quantopian score during November trading period.}\label{fig:unnamed-chunk-9}
\end{figure}

As shown, we performed relatively well in that our strategy ranked 66
out of 266 competitors on the platform. We attribute the success of this
simple strategy to its exposure to momentum, low turnover, and negative
beta, which has benefitted us in the current market where there has been
general downward trend and high volatility in asset prices.

\section{Future Improvement}\label{future-improvement}

There are a number of avenues we might consider to improve upon our
simplistic approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Building a classifier from scratch}: While we relied on what was
  provided to us by \texttt{StockTwits}, we should attempt to build our
  own classifier from possibly richer sources of messages (e.g., Weibo,
  Twitter) that can provide greater clarity on how the bull and bear
  signals arise. With a number of public posts available on how
  \texttt{StockTwits} data is utilized (Capital
  \protect\hyperlink{ref-Nairu2018}{2018}; StockTwits
  \protect\hyperlink{ref-Stocktwits2015}{2015}), there may be more
  rigorous models available that capture the bull and bear signal. For
  example, one company, Social Market Analytics, arrives at a sentiment
  score by taking the difference between the number of bullish messages
  and bearish messages of an asset and normalizes it by the total number
  of messages available (Smith \protect\hyperlink{ref-Smith2018}{2017}).
  The problem with just looking at the relative frequency of bull and
  bear messages on social media is that the data comes with its fair
  share of noise. Users each have their own agenda in which they might
  make excessive claims about a particular asset, so more sophisticated
  metrics should be designed to parse through these online market
  conversations. A possible avenue includes borrowing concepts from
  topic modeling to develop finite mixture models that arrive at a more
  reflective distribution of positive and negative sentiment of an
  asset, other unsupervised or supervised methods can also be
  considered, as well as the fact that we can now use non-trading day
  sentiment data to inform how investors feel about a stock.
\item
  \emph{Include fundamental factors}: Given the extensive literature on
  factor modeling (Fan and Yao \protect\hyperlink{ref-Fan2015}{2015}),
  we should consider including these time-tested metrics and ratios,
  like market cap and book-to-price, that have captured the financial
  characteristics of an asset. We can start with the typical (i) SMB,
  excess return of small market cap minus big market cap companies, (ii)
  HML, excess return of companies with high book-to-price against low
  book-to-price ratios, and (iii) MOM, excess return of companies that
  overperformed to those that underperformed, and study how the
  inclusion of these factors along with our sentiment index might affect
  returns. We might even find that some factors are more efficient than
  others, so swapping them in for our sentiment index might be
  necessary. Once we understand exactly how much we are exposed to
  specific factors, given the lack of predictability in the market in
  general, we should consider beta hedging to avoid any big dependencies
  on performance on a certain factor. This essentially entails taking
  the exposure \(\beta\) we have to a factor and shorting it by the
  proportional value of our total portfolio \(\beta M\).
\item
  \emph{More comprehensive exploratory data analysis}: There are basic
  steps we should have carried out prior to executing our strategy on
  \texttt{Quantopian}. Namely, we should have checked for normality,
  homoskedasticity, and autocorrelation in our model, as violations
  results in parameters that often biased, inconsistent and inefficient
  in the context of regression. If these assumptions hold, then we can
  actually normalize our factor values, allowing for them to be easily
  comparable across all assets and its different factors. If the
  assumptions hold weakly, we might consider transformations or
  techniques, like Winsorization, that will still allow us to build a
  robust statistical model for trading.
\end{enumerate}

\section{Appendix}\label{appendix}

\subsection{Quantopian Submission}\label{quantopian-submission}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Import Algorithm API functions}
\ImportTok{from}\NormalTok{ quantopian.algorithm }\ImportTok{import}\NormalTok{ (}
\NormalTok{    attach_pipeline,}
\NormalTok{    pipeline_output,}
\NormalTok{    order_optimal_portfolio,}
\NormalTok{)}
\CommentTok{# Import Optimize API module}
\ImportTok{import}\NormalTok{ quantopian.optimize }\ImportTok{as}\NormalTok{ opt}
\CommentTok{# Pipeline imports}
\ImportTok{from}\NormalTok{ quantopian.pipeline }\ImportTok{import}\NormalTok{ Pipeline}
\ImportTok{from}\NormalTok{ quantopian.pipeline.data.psychsignal }\ImportTok{import}\NormalTok{ stocktwits}
\ImportTok{from}\NormalTok{ quantopian.pipeline.factors }\ImportTok{import}\NormalTok{ SimpleMovingAverage}
\CommentTok{# Import built-in universe and Risk API method}
\ImportTok{from}\NormalTok{ quantopian.pipeline.filters }\ImportTok{import}\NormalTok{ QTradableStocksUS}
\ImportTok{from}\NormalTok{ quantopian.pipeline.experimental }\ImportTok{import}\NormalTok{ risk_loading_pipeline}
\CommentTok{# Get event data }
\ImportTok{from}\NormalTok{ quantopian.pipeline.factors.eventvestor }\ImportTok{import}\NormalTok{ (}
\NormalTok{    BusinessDaysUntilNextEarnings,}
\NormalTok{    BusinessDaysSincePreviousEarnings,}
\NormalTok{)}
\ImportTok{from}\NormalTok{ quantopian.pipeline.filters.eventvestor }\ImportTok{import}\NormalTok{ IsAnnouncedAcqTarget}
\ImportTok{from}\NormalTok{ quantopian.pipeline.factors }\ImportTok{import}\NormalTok{ BusinessDaysSincePreviousEvent}
\KeywordTok{def}\NormalTok{ initialize(context):}
    \CommentTok{# Constraint parameters}
\NormalTok{    context.max_leverage }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{    context.max_pos_size }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{    context.max_turnover }\OperatorTok{=} \FloatTok{0.8}
    \CommentTok{# Attach data pipelines}
\NormalTok{    attach_pipeline(}
\NormalTok{        make_pipeline(),}
        \StringTok{'data_pipe'}
\NormalTok{    )}
\NormalTok{    attach_pipeline(}
\NormalTok{        risk_loading_pipeline(),}
        \StringTok{'risk_pipe'}
\NormalTok{    )}
    \CommentTok{# Schedule rebalance function}
\NormalTok{    schedule_function(}
\NormalTok{        rebalance,}
\NormalTok{        date_rules.week_start(),}
\NormalTok{        time_rules.market_open(),}
\NormalTok{    )}
\KeywordTok{def}\NormalTok{ before_trading_start(context, data):}
    \CommentTok{# Get pipeline outputs and}
    \CommentTok{# store them in context}
\NormalTok{    context.output }\OperatorTok{=}\NormalTok{ pipeline_output(}\StringTok{'data_pipe'}\NormalTok{)}
\NormalTok{    context.risk_factor_betas }\OperatorTok{=}\NormalTok{ pipeline_output(}\StringTok{'risk_pipe'}\NormalTok{)}
\CommentTok{# Pipeline definition}
\KeywordTok{def}\NormalTok{ make_pipeline():}
   
\NormalTok{    not_near_earnings }\OperatorTok{=} \OperatorTok{~}\NormalTok{((BusinessDaysUntilNextEarnings() }\OperatorTok{<=} \DecValTok{2}\NormalTok{) }\OperatorTok{|}
\NormalTok{      (BusinessDaysSincePreviousEarnings() }\OperatorTok{<=} \DecValTok{2}\NormalTok{)) }
    
\NormalTok{    not_acq_tar }\OperatorTok{=} \OperatorTok{~}\NormalTok{IsAnnouncedAcqTarget()}
    
\NormalTok{    universe }\OperatorTok{=}\NormalTok{ (}
\NormalTok{        QTradableStocksUS()}
        \OperatorTok{&}\NormalTok{ not_near_earnings}
        \OperatorTok{&}\NormalTok{ not_acq_tar}
\NormalTok{    )}
    
\NormalTok{    sentiment_score }\OperatorTok{=}\NormalTok{ SimpleMovingAverage(}
\NormalTok{        inputs}\OperatorTok{=}\NormalTok{[stocktwits.bull_minus_bear],}
\NormalTok{        window_length}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{        mask}\OperatorTok{=}\NormalTok{universe}
\NormalTok{    )}
    \ControlFlowTok{return}\NormalTok{ Pipeline(}
\NormalTok{        columns}\OperatorTok{=}\NormalTok{\{}
            \StringTok{'sentiment_score'}\NormalTok{: sentiment_score,}
\NormalTok{        \},}
\NormalTok{        screen}\OperatorTok{=}\NormalTok{sentiment_score.notnull()}
\NormalTok{    )}
\KeywordTok{def}\NormalTok{ rebalance(context, data):}
    \CommentTok{# Create MaximizeAlpha objective using}
    \CommentTok{# sentiment_score data from pipeline output}
\NormalTok{    objective }\OperatorTok{=}\NormalTok{ opt.MaximizeAlpha(}
\NormalTok{      context.output.sentiment_score}
\NormalTok{    )}
    \CommentTok{# Create position size constraint}
\NormalTok{    constrain_pos_size }\OperatorTok{=}\NormalTok{ opt.PositionConcentration.with_equal_bounds(}
        \OperatorTok{-}\NormalTok{context.max_pos_size,}
\NormalTok{        context.max_pos_size}
\NormalTok{    )}
    \CommentTok{# Constrain target portfolio's leverage}
\NormalTok{    max_leverage }\OperatorTok{=}\NormalTok{ opt.MaxGrossExposure(context.max_leverage)}
    \CommentTok{# Constrain portfolio turnover}
\NormalTok{    max_turnover }\OperatorTok{=}\NormalTok{ opt.MaxTurnover(context.max_turnover)}
    \CommentTok{# Constrain target portfolio's risk exposure}
\NormalTok{    factor_risk_constraints }\OperatorTok{=}\NormalTok{ opt.experimental.RiskModelExposure(}
\NormalTok{        context.risk_factor_betas,}
\NormalTok{        version}\OperatorTok{=}\NormalTok{opt.Newest}
\NormalTok{    )}
    \CommentTok{# Rebalance portfolio using objective}
    \CommentTok{# and list of constraints}
\NormalTok{    order_optimal_portfolio(}
\NormalTok{        objective}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{        constraints}\OperatorTok{=}\NormalTok{[}
\NormalTok{            max_leverage,}
\NormalTok{            constrain_pos_size,}
\NormalTok{            max_turnover,}
\NormalTok{            factor_risk_constraints,}
\NormalTok{        ]}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\section{References}\label{references}

\noindent 

\hypertarget{refs}{}
\hypertarget{ref-Nairu2018}{}
Capital, N. (2018), ``Using google trends to predict stocks.''

\hypertarget{ref-Fan2015}{}
Fan, J., and Yao, Q. (2015), \emph{The elements of financial
econometrics}, Science Press.

\hypertarget{ref-Hong}{}
Hong, L., Harrison, and Stein, J. (2002), ``Bad news travels slowly:
Size, analyst coverage and profitability of momentum strategies,'' The
Journal of Finance, 15.

\hypertarget{ref-Jeg}{}
Jegadeesh, N., and Titman, S. (2002), ``Profitability of momentum
strategies: An evaluation of alternative explanations,'' The Journal of
Finance.

\hypertarget{ref-Quant}{}
Mackenzie, D. (2018), ``Difference between specific and total returns.''

\hypertarget{ref-Smith2018}{}
Smith, C. (2017), ``StockTwits sentiment analysis.''

\hypertarget{ref-Stocktwits2015}{}
StockTwits (2015), ``The remarkable relationship between sentiment and
your favorite stock's earnings.''
\newpage
\singlespacing 
\end{document}
